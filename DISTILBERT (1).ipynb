{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8d3fd17b1df74f5eb018a537c192e715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e137fb7c99a4cf98b8f978fe98404ea",
              "IPY_MODEL_6b1daf1511d1416d94b73d4327931844",
              "IPY_MODEL_6ee8c5e2662543a8816f9603ca5df5ca"
            ],
            "layout": "IPY_MODEL_45ebd33a25b5466d940d6dc495fc376b"
          }
        },
        "2e137fb7c99a4cf98b8f978fe98404ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_979dc6e263074737983875cbf2b307f8",
            "placeholder": "​",
            "style": "IPY_MODEL_d2c232a115444eb4b2b4519ca369b41c",
            "value": "Map: 100%"
          }
        },
        "6b1daf1511d1416d94b73d4327931844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13ff847cc59249e8a2b90178b933faf7",
            "max": 999,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c27b147841e74567b133817edcce838c",
            "value": 999
          }
        },
        "6ee8c5e2662543a8816f9603ca5df5ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_493bd74c2f6e4daf8997ca6cc8fb0363",
            "placeholder": "​",
            "style": "IPY_MODEL_340b8997e2584d609ae6f9f5817a2dbc",
            "value": " 999/999 [00:05&lt;00:00, 169.88 examples/s]"
          }
        },
        "45ebd33a25b5466d940d6dc495fc376b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "979dc6e263074737983875cbf2b307f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2c232a115444eb4b2b4519ca369b41c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13ff847cc59249e8a2b90178b933faf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c27b147841e74567b133817edcce838c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "493bd74c2f6e4daf8997ca6cc8fb0363": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "340b8997e2584d609ae6f9f5817a2dbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Using BERT (DistilBERT)** :- Training the model on a Q&A CSV Dataset for utilizing inside a Chatbot for Clinical Domain. TASK:- Question Answering."
      ],
      "metadata": {
        "id": "gdGoGpRfrNjE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 360,
          "referenced_widgets": [
            "8d3fd17b1df74f5eb018a537c192e715",
            "2e137fb7c99a4cf98b8f978fe98404ea",
            "6b1daf1511d1416d94b73d4327931844",
            "6ee8c5e2662543a8816f9603ca5df5ca",
            "45ebd33a25b5466d940d6dc495fc376b",
            "979dc6e263074737983875cbf2b307f8",
            "d2c232a115444eb4b2b4519ca369b41c",
            "13ff847cc59249e8a2b90178b933faf7",
            "c27b147841e74567b133817edcce838c",
            "493bd74c2f6e4daf8997ca6cc8fb0363",
            "340b8997e2584d609ae6f9f5817a2dbc"
          ]
        },
        "id": "s2p8N03IzXnw",
        "outputId": "a635f3dd-766e-48bb-a1aa-698dbf34d642"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/999 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d3fd17b1df74f5eb018a537c192e715"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "<ipython-input-5-38a6f95a0a16>:117: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='708' max='708' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [708/708 5:30:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "      <th>Exact Match</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.098644</td>\n",
              "      <td>0.906788</td>\n",
              "      <td>0.952381</td>\n",
              "      <td>0.952381</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.101602</td>\n",
              "      <td>0.926949</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.957143</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.360700</td>\n",
              "      <td>0.104184</td>\n",
              "      <td>0.926949</td>\n",
              "      <td>0.957143</td>\n",
              "      <td>0.957143</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=708, training_loss=0.26330079197210104, metrics={'train_runtime': 19894.4927, 'train_samples_per_second': 0.285, 'train_steps_per_second': 0.036, 'total_flos': 740019158581248.0, 'train_loss': 0.26330079197210104, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "####################  TRAINER for BERT / DISTILBERT / ROBERTA / ClinicalBERT / MedBERT   with CSV DATASET   ###############################\n",
        "\n",
        "import os\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import Dataset, DatasetDict, load_metric\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "\n",
        "# Load and sample dataset\n",
        "df = pd.read_csv('/content/CancerQA.csv')\n",
        "df.rename(columns={'Question': 'question', 'Answer': 'context'}, inplace=True)\n",
        "#df = df.sample(n=700, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Tokenizer and model checkpoint\n",
        "model_checkpoint = \"distilbert-base-uncased\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
        "\n",
        "max_length = 512\n",
        "stride = 128\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess_examples(examples):\n",
        "    questions = [str(q) for q in examples['question']]\n",
        "    contexts = [str(c) for c in examples['context']]\n",
        "\n",
        "    tokenized_inputs = tokenizer(\n",
        "        questions,\n",
        "        contexts,\n",
        "        max_length=max_length,\n",
        "        truncation=\"only_second\",\n",
        "        stride=stride,\n",
        "        return_overflowing_tokens=True,\n",
        "        return_offsets_mapping=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    sample_mapping = tokenized_inputs.pop(\"overflow_to_sample_mapping\")\n",
        "    offset_mapping = tokenized_inputs.pop(\"offset_mapping\")\n",
        "\n",
        "    start_positions = []\n",
        "    end_positions = []\n",
        "\n",
        "    for i, offsets in enumerate(offset_mapping):\n",
        "        sample_index = sample_mapping[i]\n",
        "        context = str(examples['context'][sample_index]).lower()\n",
        "        answer = context  # assuming full context is the answer\n",
        "\n",
        "        start_char = context.find(answer)\n",
        "        end_char = start_char + len(answer)\n",
        "\n",
        "        start_pos = end_pos = 0  # default fallback\n",
        "        for idx, (start, end) in enumerate(offsets):\n",
        "            if start_char >= start and start_char < end:\n",
        "                start_pos = idx\n",
        "            if end_char > start and end_char <= end:\n",
        "                end_pos = idx\n",
        "\n",
        "        start_positions.append(start_pos)\n",
        "        end_positions.append(end_pos)\n",
        "\n",
        "    tokenized_inputs.update({\n",
        "        'start_positions': start_positions,\n",
        "        'end_positions': end_positions\n",
        "    })\n",
        "    return tokenized_inputs\n",
        "\n",
        "# Convert to Hugging Face Dataset\n",
        "dataset = Dataset.from_pandas(df)\n",
        "dataset = dataset.map(preprocess_examples, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "# Split dataset\n",
        "train_test_split = dataset.train_test_split(test_size=0.1)\n",
        "dataset = DatasetDict({\n",
        "    'train': train_test_split['train'],\n",
        "    'validation': train_test_split['test']\n",
        "})\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
        "\n",
        "# Define metrics\n",
        "def compute_metrics(p):\n",
        "    pred_start = torch.argmax(torch.tensor(p.predictions[0]), dim=1)\n",
        "    pred_end = torch.argmax(torch.tensor(p.predictions[1]), dim=1)\n",
        "    true_start = torch.tensor(p.label_ids[0])\n",
        "    true_end = torch.tensor(p.label_ids[1])\n",
        "\n",
        "    # F1 and EM for positions\n",
        "    exact_match = (pred_start == true_start) & (pred_end == true_end)\n",
        "    f1 = f1_score(true_start.cpu(), pred_start.cpu(), average='macro')\n",
        "    acc = accuracy_score(true_start.cpu(), pred_start.cpu())\n",
        "\n",
        "    return {\n",
        "        'f1': f1,\n",
        "        'exact_match': exact_match.float().mean().item(),\n",
        "        'accuracy': acc,\n",
        "    }\n",
        "\n",
        "# Training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    run_name=\"CancerQA-DistilBERT-Run11\",\n",
        "    eval_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    load_best_model_at_end=True,\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    report_to=\"none\",\n",
        "    logging_dir='./logs',\n",
        ")\n",
        "\n",
        "# Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['validation'],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Train\n",
        "trainer.train()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p4oPq5uMtAWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. (8GB RAM / No GPU or CUDA support), for a Medical Q&A dataset from Kaggle of 16K records it took more than 4 hours to go from 0.00 to 0.01/3 of the 3 Epochs.\n",
        "2. used \"google-bert/bert-base-uncased\" with this 16K rows. It ran for more than 4 hours with very very slow progress.\n",
        "3. shifted to google colab for its GPU/TPU for faster computations and RAM. Even then with a GPU, shifted from BERT 110 Million parameters to DistilBERT 66 Million for ~40% faster Training Time. (Because the Session would crash and Restart on Colab, and afte some time the Utility for GPU/TPU would expire)\n",
        "4. With only a 1000 records and DistilBERT it finally Trained and saved the model after 3 Epochs.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "60-hSUH0tD4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#############  Script to Load Trained Model and Run Inference  ###########################\n",
        "#############  Module with Basic Start-End-Token Handling   ###########################\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer from best checkpoint\n",
        "model_path = \"/content/drive/MyDrive/DistilBERT_Trained/checkpoint-708\"  # <-- Replace with actual best checkpoint path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    start_idx = torch.argmax(outputs.start_logits)\n",
        "    end_idx = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx])\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"what is tinnitus?\"\n",
        "context = \"Tinnitus is the perception of ringing, buzzing, hissing, or other sounds in the ears or head when no external sound is present. Key Facts: It’s not a disease, but a symptom of an underlying condition. Commonly described as ringing in the ears, though some may hear clicking, roaring, or whooshing sounds. Can be intermittent or constant, and vary in loudness.\"\n",
        "print(\"Answer:\", answer_question(question, context))\n"
      ],
      "metadata": {
        "id": "2WAfkZuqzoDS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8feb3c3f-bffa-4db2-9e1c-551365b3ef84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: tinnitus is the perception of ringing, buzzing, hissing, or other sounds in the ears or head when no external sound is present. key facts : it ’ s not a disease, but a symptom of an underlying condition. commonly described as ringing in the ears, though some may hear clicking, roaring, or whooshing sounds. can be intermittent or constant, and vary in loudness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " It detects the Start_Token_ID and End_Token_ID +1 ID from context corpus to only extract the answer and not Generate, only after training on a dataset with SQuAD (JSON) formatted dataset or equivalent CSV formating.\n",
        "\n",
        "1.   Though it doesn't understand \"NO\" or \"Invalid Question or Answer\" for as an output answer, even if the Question and the Context are irrelevant, or even if a single token is passed as the context corpus. It will just try to fetch something as an answer and return as the output.\n",
        "2.   Fine tuning a pre-trained BERT model with gpt-2 would be great for low resource chatbot implementation.\n",
        "\n"
      ],
      "metadata": {
        "id": "PnaXIGGEs8lF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o-lsOi8scRwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mZ8DwQpXcRzV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#############  Script to Load Trained Model and Run Inference  ###########################\n",
        "#############  Module with robust Start-End-Token Handling   ###########################\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer from best checkpoint\n",
        "model_path = \"/content/drive/MyDrive/DistilBERT_Trained/checkpoint-708\"  # <-- Replace with actual best checkpoint path\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
        "    input_ids = inputs['input_ids'][0]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Get start and end token scores\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "\n",
        "    # Get most likely beginning and end of answer with .item()\n",
        "    start_idx = torch.argmax(start_scores)\n",
        "    end_idx = torch.argmax(end_scores)\n",
        "\n",
        "    # Ensure the end is after the start\n",
        "    if start_idx > end_idx:\n",
        "        end_idx = start_idx\n",
        "\n",
        "    answer_ids = input_ids[start_idx:end_idx+1]\n",
        "    answer = tokenizer.decode(answer_ids, skip_special_tokens=True)\n",
        "\n",
        "    return answer.strip()\n",
        "\n",
        "\n",
        "# Example usage\n",
        "question = \"What is Tinnitus?\"\n",
        "context = \"Tinnitus is the perception of ringing, buzzing, hissing, or other sounds in the ears or head when no external sound is present. Key Facts: It’s not a disease, but a symptom of an underlying condition. Commonly described as ringing in the ears, though some may hear clicking, roaring, or whooshing sounds. Can be intermittent or constant, and vary in loudness.\"\n",
        "print(\"Answer:\", answer_question(question, context))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kz0PzTj0cVQd",
        "outputId": "92198739-47ba-48ce-90e6-e3014f378af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: tinnitus is the perception of ringing, buzzing, hissing, or other sounds in the ears or head when no external sound is present. key facts : it ’ s not a disease, but a symptom of an underlying condition. commonly described as ringing in the ears, though some may hear clicking, roaring, or whooshing sounds. can be intermittent or constant, and vary in loudness.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nzQYOeqCyxMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f16XvqavyxeO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(\"/content/drive/MyDrive/DistilBERT_Trained/checkpoint-708\") #medicalai/ClinicalBERT    #emilyalsentzer/Bio_ClinicalBERT\n",
        "print(model.base_model_prefix)\n",
        "print(model.num_parameters())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUJz-7DMcVTL",
        "outputId": "e8465c29-a456-4a26-fb55-9d00b2aa0b9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "distilbert\n",
            "66364418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering\n",
        "\n",
        "# Load model\n",
        "model_path = \"/content/drive/MyDrive/DistilBERT_Trained/checkpoint-708\"\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "# Check trainable parameters\n",
        "trainable_params = [p for p in model.parameters() if p.requires_grad]\n",
        "total_params = sum(p.numel() for p in trainable_params)\n",
        "\n",
        "if total_params == 0:\n",
        "    print(\"✅ All parameters are frozen. No trainable parameters left.\")\n",
        "else:\n",
        "    print(f\"⚠️ There are still trainable parameters. Total trainable: {total_params}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWJZDYhQgOrl",
        "outputId": "799399ac-a168-4313-a808-5b4dcc3acfac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⚠️ There are still trainable parameters. Total trainable: 66364418\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RfDvBCUAcVV-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "id": "DeX0PotQTzR5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##########################   UI for Q&A Testing  ##########################\n",
        "import gradio as gr\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_path = \"/content/drive/MyDrive/DistilBERT_Trained/checkpoint-708\"  # Replace checkpoint folder  #emilyalsentzer/Bio_ClinicalBERT   #medicalai/ClinicalBERT\n",
        "#################  # Replace checkpoint folder  #emilyalsentzer/Bio_ClinicalBERT   #medicalai/ClinicalBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "\n",
        "def get_answer(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    start_idx = torch.argmax(outputs.start_logits)\n",
        "    end_idx = torch.argmax(outputs.end_logits) + 1\n",
        "    answer = tokenizer.convert_tokens_to_string(\n",
        "        tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx])\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "gr.Interface(\n",
        "    fn=get_answer,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, label=\"Question\"),\n",
        "        gr.Textbox(lines=5, label=\"Context Paragraph\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"DistilBERT Medical QA\",\n",
        "    description=\"Ask a question based on the provided medical context.\"\n",
        ").launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "vGn7QL1CzoFj",
        "outputId": "d947c49c-2a50-41f2-cc1b-fa29250b17ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f35c45b83eb5bc34f2.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f35c45b83eb5bc34f2.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tHpKjondp82j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cIqrwO-fzeKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit -q\n",
        "!pip install pyngrok -q"
      ],
      "metadata": {
        "id": "pB9eMgWqp85S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
        "import torch\n",
        "from pyngrok import ngrok\n",
        "\n",
        "ngrok.kill()\n",
        "\n",
        "NGROK_AUTH_TOKEN = \"YOUR_AUTHTOKEN\"\n",
        "ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n",
        "\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit App URL: {public_url}\")\n",
        "\n",
        "\n",
        "model_path = \"/content/drive/MyDrive/DistilBERT_Trained/checkpoint-708\"\n",
        "\n",
        "@st.cache_resource\n",
        "def load_qa_model(model_path):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "    model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
        "    return tokenizer, model\n",
        "\n",
        "# Load the model\n",
        "tokenizer, model = load_qa_model(model_path)\n",
        "\n",
        "def answer_question(question, context):\n",
        "    inputs = tokenizer(question, context, return_tensors='pt', truncation=True, max_length=512)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        start_idx = torch.argmax(outputs.start_logits)\n",
        "        end_idx = torch.argmax(outputs.end_logits) + 1\n",
        "        answer = tokenizer.convert_tokens_to_string(\n",
        "            tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][start_idx:end_idx])\n",
        "        )\n",
        "        return answer\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"DistilBERT Medical QA\")\n",
        "st.write(\"Ask a question based on the provided medical context.\")\n",
        "\n",
        "question = st.text_input(\"Question:\", \"\")\n",
        "context = st.text_area(\"Context Paragraph:\", \"\")\n",
        "\n",
        "if st.button(\"Get Answer\"):\n",
        "    if question and context:\n",
        "        answer = answer_question(question, context)\n",
        "        st.subheader(\"Answer:\")\n",
        "        st.write(answer)\n",
        "    else:\n",
        "        st.warning(\"Please enter both a question and context.\")\n",
        "\n",
        "\n",
        "!streamlit run app.py &>/dev/null&\n",
        "print(f\"Streamlit App URL: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWV9lKncZYyi",
        "outputId": "7fc990d2-0f4f-413d-c90a-4f37fabebaeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Studying and Researching BERT models with hands on experimentations has profoundly deepen my knowledge regarding the powerful Bi-Directional Encoder with attention span Part of the Transformers.\n",
        "Not only that but also how to choose a system hardware setup to train larger models with larger and complex datasets as well.\n",
        "Under the guidance of our NLP Professor Abdul sir from DESPU Pune, this has been a great learning experiance with new technologies.\n",
        "Thank You.\n",
        "-Siddhant Mutha (MSc. DS , SY)"
      ],
      "metadata": {
        "id": "TWT8czAwzkHN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MPjmj1DMXoEc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J_AJ9rZQXoHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nKZMByiWXoJp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}